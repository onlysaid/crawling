{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlwOhvXO49+NCXnmZirdSv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/onlysaid/crawling/blob/main/tugas_11b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xatJAlktIS-P",
        "outputId": "7cf8a2e8-8366-45f7-dabc-9b631e09a369"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Proses Crawling Dimulai Mohon Tunggu...\n",
            "Proses Crawling Selesai. Data disimpan di dataHiro_crawling.csv\n"
          ]
        }
      ],
      "source": [
        "# Instal library tambahan jika belum ada\n",
        "# !pip install beautifulsoup4 requests\n",
        "\n",
        "# Import library yang diperlukan\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "\n",
        "# URL yang ingin di-crawl\n",
        "url = 'https://hirogroup.co.id/brands/eat-drink/'\n",
        "\n",
        "# Inisialisasi CSV writer\n",
        "csvFile = open('dataHiro_crawling.csv', 'w', newline='', encoding='utf-8')\n",
        "csvWriter = csv.writer(csvFile)\n",
        "\n",
        "# Header untuk file CSV\n",
        "csvWriter.writerow(['Title', 'Content'])\n",
        "\n",
        "# Melakukan request ke URL\n",
        "response = requests.get(url)\n",
        "\n",
        "# Cek apakah request berhasil\n",
        "if response.status_code == 200:\n",
        "    print('Proses Crawling Dimulai Mohon Tunggu...')\n",
        "\n",
        "    # Parsing konten menggunakan BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Contoh: Mengambil semua paragraf dalam halaman tersebut\n",
        "    title = soup.title.string if soup.title else 'No Title'\n",
        "    paragraphs = soup.find_all('p')\n",
        "\n",
        "    # Menulis data ke CSV\n",
        "    for p in paragraphs:\n",
        "        content = p.text.strip()\n",
        "        if content:  # Menulis hanya jika ada konten\n",
        "            csvWriter.writerow([title, content])\n",
        "\n",
        "    print('Proses Crawling Selesai. Data disimpan di dataHiro_crawling.csv')\n",
        "else:\n",
        "    print(f'Gagal mengakses halaman. Status code: {response.status_code}')\n",
        "\n",
        "# Menutup file CSV\n",
        "csvFile.close()\n"
      ]
    }
  ]
}